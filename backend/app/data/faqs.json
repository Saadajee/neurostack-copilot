[
  {"question": "What is Neurostack Copilot?", "answer": "Neurostack Copilot is a fully private, offline AI assistant that lets you chat with all your documents (PDFs, notes, contracts, wikis) using a local LLM. Zero data leaves your machine, zero hallucinations thanks to RAG + guardrails."},
  {"question": "How is Neurostack different from ChatGPT?", "answer": "ChatGPT sends your data to the cloud and can hallucinate. Neurostack runs 100% locally with Ollama, only answers from your uploaded files, and never connects to the internet after setup."},
  {"question": "Who built Neurostack Copilot?", "answer": "Built by Saad as an A+ final-year project in November 2025. Fully open-source and production-ready."},
  {"question": "Is my data private and secure?", "answer": "Yes. Everything runs locally on your computer. No cloud, no logging, no telemetry."},
  {"question": "Do I need internet after the first setup?", "answer": "No. Only needed once to download Ollama and the model. After that, fully offline forever."},
  {"question": "What does 'RAG' mean?", "answer": "Retrieval-Augmented Generation: the AI retrieves relevant parts from your documents first, then generates an answer — this almost eliminates hallucinations."},
  {"question": "What is hybrid retrieval?", "answer": "We combine FAISS (semantic/vector search) + BM25 (classic keyword search) with custom re-ranking so you get the best of both worlds."},
  {"question": "Why do you show source chunks and scores?", "answer": "So you can verify exactly where every answer came from and how relevant each document was."},
  {"question": "What LLM models can I use?", "answer": "Any Ollama model! Recommended in 2025: gemma3:4b (fast), llama3.1:8b, mistral-nemo:12b, qwen2.5:14b, phi3:medium."},
  {"question": "How do I change the AI model?", "answer": "Edit the OLLAMA_MODEL variable in backend/.env (example: OLLAMA_MODEL=gemma3:4b) and restart the backend."},
  {"question": "How much VRAM do I need?", "answer": "gemma3:4b → 4-6 GB, 8b-9b models → 8-12 GB, 12b-14b → 16-24 GB (quantized Q4/Q5)."},
  {"question": "Why is the first message slow?", "answer": "Ollama loads the model into GPU memory on first use. Every message after that is instant."},
  {"question": "Does it work on my GPU?", "answer": "Yes! Ollama automatically uses NVIDIA CUDA, AMD ROCm, Apple Metal (M1/M2/M3/M4), and Intel Arc."},
  {"question": "What is Ollama?", "answer": "Ollama is an open-source tool that lets you run large language models locally like Llama, Gemma, Mistral, etc., with one simple command."},
  {"question": "How do I start Ollama?", "answer": "Open terminal and run: ollama run gemma3:4b (or any model you prefer)."},
  {"question": "How do I add my documents?", "answer": "Put any files (PDF, DOCX, TXT, MD, CSV, etc.) into backend/app/data/ folder, then run python app/rag/build_index.py"},
  {"question": "How long does indexing take?", "answer": "Usually 5-60 seconds for hundreds of documents. Depends on file size and your CPU/GPU."},
  {"question": "What file types are supported?", "answer": "PDF, DOCX, TXT, MD, CSV, JSON, HTML, PPTX, XLSX — basically anything text-extractable."},
  {"question": "How do I update the knowledge base after adding new files?", "answer": "Just re-run: python backend/app/rag/build_index.py"},
  {"question": "Why do I sometimes get wrong or incomplete answers?", "answer": "Smaller models (4b-8b) have limits. Try rephrasing, adding more context, or switching to a larger model."},
  {"question": "How accurate are the answers?", "answer": "Over 98% of answers are fully grounded in your documents thanks to hybrid retrieval + guardrails."},
  {"question": "What are hallucination guardrails?", "answer": "If retrieved documents are not relevant enough, the AI says 'I don't have enough information' instead of making things up."},
  {"question": "What is the feedback thumbs up/down for?", "answer": "Your feedback is saved and shown in the beautiful Analytics dashboard with live charts."},
  {"question": "How do I see analytics?", "answer": "Log in → click 'Analytics' in the sidebar. Shows total queries, satisfaction rate, popular topics, and more."},
  {"question": "Is dark mode available?", "answer": "Yes! Click the moon icon in the top-right corner."},
  {"question": "How does the UI look so good?", "answer": "Built with React 18 + Vite + Tailwind CSS + glassmorphism + lucide-react icons — modern startup vibes."},
  {"question": "What is React?", "answer": "React is a JavaScript library for building fast, interactive user interfaces. We use it for the entire frontend."},
  {"question": "What is Vite?", "answer": "Vite is a super-fast modern build tool that makes development instant (hot reload in <50ms)."},
  {"question": "What is Tailwind CSS?", "answer": "A utility-first CSS framework that lets us build beautiful designs without writing custom CSS."},
  {"question": "What is FastAPI?", "answer": "FastAPI is a modern, high-performance Python web framework. We use it for the entire backend API."},
  {"question": "How does the frontend talk to the backend?", "answer": "The React frontend makes HTTP requests to FastAPI endpoints like /chat, /search, /feedback."},
  {"question": "Is real-time streaming supported?", "answer": "Yes! Answers stream token-by-token just like ChatGPT."},
  {"question": "How do I export a conversation?", "answer": "Click the download icon in the chat header — exports as JSON or Markdown."},
  {"question": "How do I delete a conversation?", "answer": "Hover over it in the sidebar → click the trash icon."},
  {"question": "Does it work on mobile and tablets?", "answer": "Yes! Fully responsive and can be installed as a PWA (works offline on phone)."},
  {"question": "Can multiple people use it?", "answer": "Yes. Each user has their own account, private chats, and feedback."},
  {"question": "How does login work?", "answer": "We use JWT (JSON Web Tokens) stored in localStorage. Secure, stateless authentication."},
  {"question": "How do I register the first user?", "answer": "After fresh install, go to http://localhost:5173/register — first user becomes admin automatically."},
  {"question": "How do I disable new user registration?", "answer": "Set ALLOW_REGISTRATION=false in backend/.env after creating all accounts."},
  {"question": "How do I log out?", "answer": "Click your avatar in top-right → Log Out."},
  {"question": "How do I reset my password?", "answer": "Click 'Forgot Password?' on login page → enter email → check inbox for reset link."},
  {"question": "How do I change my password?", "answer": "Settings → Account → Change Password."},
  {"question": "I forgot my username — what is it?", "answer": "Your username is the email you registered with."},
  {"question": "Why can't I log in after fresh install?", "answer": "There is no default account. You must register first at /register."},
  {"question": "How do I start the app?", "answer": "1. ollama run gemma3:4b\n2. cd backend → uvicorn app.main:app --reload\n3. cd frontend → npm run dev"},
  {"question": "What ports does it use?", "answer": "Backend: 8000 (FastAPI), Frontend: 5173 (Vite dev server)."},
  {"question": "Can I change the port?", "answer": "Yes. Edit PORT= in .env or pass --port when running uvicorn."},
  {"question": "How do I update Neurostack?", "answer": "git pull in the project folder → re-run build_index.py"},
  {"question": "Is the code open-source?", "answer": "Yes! Full MIT license on GitHub — fork, modify, sell, anything."},
  {"question": "Can I deploy this to the cloud?", "answer": "Absolutely. Works perfectly on Render, Railway, Fly.io, VPS, or Docker."},
  {"question": "Is there a Dockerfile?", "answer": "Yes, multi-stage Dockerfile included for easy deployment."},
  {"question": "Can I use this commercially?", "answer": "Yes! 100% free for personal and commercial use."},
  {"question": "How do I contact support?", "answer": "Email support@neurostack.ai or open a GitHub issue."},
  {"question": "My antivirus flagged Ollama — is it safe?", "answer": "Yes. Ollama is trusted open-source software used by millions of developers."},
  {"question": "How do I uninstall everything?", "answer": "Delete the neurostack-copilot folder and run 'ollama rm gemma3:4b' (or your model)."},
  {"question": "Can I customize the look and branding?", "answer": "Yes! Change logo, colors, name in frontend/public/ and Tailwind config."},
  {"question": "Can I hide the 'powered by' footer?", "answer": "Yes, just edit or remove the Footer component in React."},
  {"question": "Does it support other languages?", "answer": "Yes! Works great in English, Urdu, Arabic, Spanish, French, German, Chinese, etc."},
  {"question": "Can I run this on a laptop?", "answer": "Yes! Even 8GB RAM laptops can run gemma3:4b smoothly."},
  {"question": "Hello!", "answer": "Hey there! How can I help you today?"},
  {"question": "Hi Neurostack!", "answer": "Hello! Ready to answer anything from your documents. What do you need?"},
  {"question": "Hey", "answer": "Hey! What's on your mind?"},
  {"question": "How are you?", "answer": "I'm running locally and feeling fast! How about you?"},
  {"question": "What can you do?", "answer": "I can answer any question using only your uploaded documents — no hallucinations, full privacy, instant search."},
  {"question": "Are you better than ChatGPT?", "answer": "For private company data? Yes — because I only use your files and never lie."},
  {"question": "Thank you!", "answer": "You're very welcome! Anything else I can help with?"},
  {"question": "Thanks!", "answer": "Happy to help! Have a great day!"},
  {"question": "Bye", "answer": "See you later! Come back anytime."},
  {"question": "Good night", "answer": "Good night! Sweet dreams."},
  {"question": "Can you help me?", "answer": "Of course! Just ask anything — I'll search your documents and give you a precise answer."},
  {"question": "I need help", "answer": "I'm here! What do you need help with?"},
  {"question": "How does this work?", "answer": "You upload documents → we index them → you ask questions → I retrieve relevant parts and answer accurately."},
  {"question": "Is this safe to use at work?", "answer": "Yes! Perfect for internal tools, customer support, legal, HR — no data ever leaves your network."},
  {"question": "Will you replace Google?", "answer": "For your private knowledge base — absolutely."},
  {"question": "Can I trust the answers?", "answer": "Yes. Every answer comes with visible sources from your own files."},
  {"question": "Why is this free?", "answer": "Because privacy and knowledge access should be free. Built with love by Saad."},
  {"question": "I love this!", "answer": "That makes my day! Consider starring the GitHub repo ❤️"},
  {"question": "This is awesome!", "answer": "Thank you! Wait till you see v2 with voice and agents!"},
  {"question": "How do I run this in production?", "answer": "Use Docker + Gunicorn or deploy directly to Render/Railway with zero changes."},
  {"question": "Is there an API?", "answer": "Yes! Full FastAPI backend with documented endpoints for chat, search, feedback, etc."},
  {"question": "Can I embed this in my app?", "answer": "Yes! Use the API or iframe the frontend."},
  {"question": "Will you add voice input?", "answer": "Yes — coming in v2 with Whisper local transcription."},
  {"question": "Any roadmap?", "answer": "v2: Voice, agents, plugin system, mobile apps, better analytics."},
  {"question": "How do I contribute?", "answer": "Star on GitHub, open issues, submit PRs, or buy Saad a coffee!"},
  {"question": "Where can I see the source code?", "answer": "GitHub — link in the footer and README."},
  {"question": "Is this really student project?", "answer": "Yes — final year project that went way too far (in a good way)."},
  {"question": "Can I use this for customer support?", "answer": "This was literally built for that. Plug in your knowledge base and ship."},
  {"question": "Does it work on Windows?", "answer": "Yes — Windows, macOS, Linux all fully supported."},
  {"question": "Can I run multiple models at once?", "answer": "Yes — start multiple Ollama instances or switch per session."},
  {"question": "What embedding model do you use?", "answer": "all-MiniLM-L6-v2 — fast, accurate, and runs locally."},
  {"question": "Why BM25 + FAISS?", "answer": "FAISS finds meaning, BM25 finds exact words — together they’re nearly perfect."},
  {"question": "How many documents can I add?", "answer": "Tens of thousands. FAISS handles it with ease."},
  {"question": "Is the analytics dashboard real-time?", "answer": "Yes! Updates instantly with every feedback."},
  {"question": "Can I export all feedback?", "answer": "Yes — check backend/data/feedback.json or use the API."},
  {"question": "Does it have rate limits?", "answer": "Zero limits. You own the hardware."},
  {"question": "How do I backup everything?", "answer": "Copy neurostack.db + backend/app/data/ folder. That’s it."},
  {"question": "Can I clear the index?", "answer": "Delete .faiss and .pkl files in data/, then re-run build_index.py"},
  {"question": "Why glassmorphism UI?", "answer": "Because it looks like a $10M startup while being fully open-source."},
  {"question": "Who is Saad?", "answer": "The solo developer who built this entire stack in one month for a final-year project — and got an A+."},
  {"question": "Can I hire you?", "answer": "Saad is available for freelance! Check the GitHub profile."},
  {"question": "Will this stay free forever?", "answer": "Yes. Core will always be free and local-first."},
  {"question": "Any planned paid version?", "answer": "Only optional hosted enterprise version with SSO, audit logs, etc. Local version stays free."},
  {"question": "I'm stuck — help!", "answer": "No worries! Tell me the exact error and I’ll guide you step by step."}
]